---
---

@article{saycam-text,
    title = "Finding Structure in One Child's Linguistic Experience",
    author = "Wang, Wentao and Vong, Wai Keen and Kim, Najoung and Brenden M. Lake",
    journal = "Cognitive Science",
    url = "https://psyarxiv.com/85k3y",
    pdf = "https://psyarxiv.com/85k3y",
	bibtex_show = true,
	abstract = "Neural network models have recently made striking progress in natural language processing, but they are typically trained on orders of magnitude more language input than children receive. What can these neural networks, which are primarily distributional learners, learn from a naturalistic subset of a single child's experience? We examine this question using a recent longitudinal dataset collected from a single child, consisting of egocentric visual data paired with text transcripts. We train both language-only and vision-and-language neural networks and analyze the linguistic knowledge they acquire. In parallel with findings from Elman's (1990) seminal work, the neural networks form emergent clusters of words corresponding to syntactic (nouns, transitive and intransitive verbs) and semantic categories (e.g., animals and clothing), based solely on one child's linguistic input. The networks also acquire sensitivity to acceptability contrasts from linguistic phenomena such as determiner-noun agreement and argument structure. We find that incorporating visual information produces an incremental gain in predicting words in context, especially for syntactic categories that are comparatively more easily grounded such as nouns and verbs, but the underlying linguistic representations are not fundamentally altered. Our findings demonstrate which kinds of linguistic knowledge are learnable from a snapshot of a single child's real developmental experience.",
	year = "2023",
	kind = "2023",
}

@article{saycam-multimodal,
    title = "Grounded language acquisition through the eyes and ears of a single child",
    author = "Wai Keen Vong and Wentao Wang and A. Emin Orhan and Brenden M. Lake",
	journal = "(Under review) Science",
	kind = "In prep.",
}

@inproceedings{lin-etal-2020-data,
    title = "Data-to-Text Generation with Style Imitation",
    author = "Lin, Shuai  and
      Wang, Wentao  and
      Yang, Zichao  and
      Liang, Xiaodan  and
      Xu, Frank F.  and
      Xing, Eric  and
      Hu, Zhiting",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.144",
    doi = "10.18653/v1/2020.findings-emnlp.144",
    pages = "1589--1598",
    abstract = "Recent neural approaches to data-to-text generation have mostly focused on improving content fidelity while lacking explicit control over writing styles (e.g., sentence structures, word choices). More traditional systems use templates to determine the realization of text. Yet manual or automatic construction of high-quality templates is difficult, and a template acting as hard constraints could harm content fidelity when it does not match the record perfectly. We study a new way of stylistic control by using existing sentences as {``}soft{''} templates. That is, a model learns to imitate the writing style of any given exemplar sentence, with automatic adaptions to faithfully describe the record. The problem is challenging due to the lack of parallel data. We develop a neural approach that includes a hybrid attention-copy mechanism, learns with weak supervisions, and is enhanced with a new content coverage constraint. We conduct experiments in restaurants and sports domains. Results show our approach achieves stronger performance than a range of comparison methods. Our approach balances well between content fidelity and style control given exemplars that match the records to varying degrees.",
	bibtex_show = true,
	pdf = "https://aclanthology.org/2020.findings-emnlp.144.pdf",
	kind = "Older",
}

@inproceedings{hu-etal-2019-texar,
    title = "{T}exar: A Modularized, Versatile, and Extensible Toolkit for Text Generation",
    author = "Hu, Zhiting  and
      Shi, Haoran  and
      Tan, Bowen  and
      Wang, Wentao  and
      Yang, Zichao  and
      Zhao, Tiancheng  and
      He, Junxian  and
      Qin, Lianhui  and
      Wang, Di  and
      Ma, Xuezhe  and
      Liu, Zhengzhong  and
      Liang, Xiaodan  and
      Zhu, Wanrong  and
      Sachan, Devendra  and
      Xing, Eric",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-3027",
    doi = "10.18653/v1/P19-3027",
    pages = "159--164",
    abstract = "We introduce Texar, an open-source toolkit aiming to support the broad set of text generation tasks that transform any inputs into natural language, such as machine translation, summarization, dialog, content manipulation, and so forth. With the design goals of modularity, versatility, and extensibility in mind, Texar extracts common patterns underlying the diverse tasks and methodologies, creates a library of highly reusable modules and functionalities, and allows arbitrary model architectures and algorithmic paradigms. In Texar, model architecture, inference, and learning processes are properly decomposed. Modules at a high concept level can be freely assembled or plugged in/swapped out. Texar is thus particularly suitable for researchers and practitioners to do fast prototyping and experimentation. The versatile toolkit also fosters technique sharing across different text generation tasks. Texar supports both TensorFlow and PyTorch, and is released under Apache License 2.0 at https://www.texar.io.",
	bibtex_show = true,
	pdf = "https://aclanthology.org/P19-3027.pdf",
	kind = "Older",
}
